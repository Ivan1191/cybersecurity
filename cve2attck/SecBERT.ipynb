{"cells":[{"cell_type":"markdown","metadata":{"id":"CrpV1mQ2A-MI"},"source":["## **Multi-label SecBERT**\n","https://www.mdpi.com/1999-4893/15/9/314"]},{"cell_type":"markdown","metadata":{"id":"aMKyZcnVBbvj"},"source":["### Import environment"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface-hub==0.16.4 in /home/ismp/.local/lib/python3.8/site-packages (0.16.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub==0.16.4) (5.3.1)\n","Requirement already satisfied: packaging>=20.9 in /home/ismp/.local/lib/python3.8/site-packages (from huggingface-hub==0.16.4) (23.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /home/ismp/.local/lib/python3.8/site-packages (from huggingface-hub==0.16.4) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.16.4) (4.4.0)\n","Requirement already satisfied: filelock in /home/ismp/.local/lib/python3.8/site-packages (from huggingface-hub==0.16.4) (3.12.4)\n","Requirement already satisfied: requests in /home/ismp/.local/lib/python3.8/site-packages (from huggingface-hub==0.16.4) (2.31.0)\n","Requirement already satisfied: fsspec in /home/ismp/.local/lib/python3.8/site-packages (from huggingface-hub==0.16.4) (2023.9.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub==0.16.4) (1.25.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub==0.16.4) (2019.11.28)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/ismp/.local/lib/python3.8/site-packages (from requests->huggingface-hub==0.16.4) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub==0.16.4) (2.8)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install huggingface-hub==0.16.4"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5608,"status":"ok","timestamp":1697683149411,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"Ch-anuOEB73v"},"outputs":[],"source":["!pip install -q transformers"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: matplotlib in /home/ismp/.local/lib/python3.8/site-packages (3.7.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib) (23.0)\n","Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: cycler>=0.10 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n","Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/ismp/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.11.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install matplotlib"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: seaborn in /home/ismp/.local/lib/python3.8/site-packages (0.13.0)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /home/ismp/.local/lib/python3.8/site-packages (from seaborn) (3.7.3)\n","Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.23.5)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.5.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n","Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.1.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (7.0.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.43.1)\n","Requirement already satisfied: packaging>=20.0 in /home/ismp/.local/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2->seaborn) (2022.6)\n","Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/ismp/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib!=3.6.1,>=3.3->seaborn) (3.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.14.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install seaborn"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1697683149412,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"fvVqyZrbByWi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import copy\n","from tqdm.notebook import tqdm\n","import gc\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.metrics import (\n","    accuracy_score,\n","    recall_score,\n","    precision_score,\n","    multilabel_confusion_matrix,\n","    f1_score,\n","    classification_report\n",")\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModel,\n","    get_linear_schedule_with_warmup\n",")\n","\n","from transformers import BertTokenizer\n"]},{"cell_type":"markdown","metadata":{"id":"eyD-w8YLCOby"},"source":["### Load the dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":478,"status":"ok","timestamp":1697683149886,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"hHyUzRBLCfNo"},"outputs":[],"source":["X_train = pd.read_csv('X_train.csv')\n","y_train = pd.read_csv('y_train.csv')\n","\n","X_test = pd.read_csv('X_test.csv')\n","y_test = pd.read_csv('y_test.csv')\n","\n","y_train = y_train.astype(int)\n","y_test = y_test.astype(int)\n","\n","our_X_test = pd.read_csv('our_X_test.csv')"]},{"cell_type":"markdown","metadata":{"id":"nK9xE2XbBn3K"},"source":["### Auxiliary Functions"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1697683149886,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"b3ajHUrnBtNn"},"outputs":[],"source":["def text_processing(text):\n","  doc = nlp(text)\n","  # Extract tokens for the given doc\n","  tokens = [token.lemma_ for token in doc if not (token.is_punct or token.is_space or token.is_stop) ]\n","  return tokens\n","\n","def text_processing_keep_nouns_verbs(text):\n","  doc = nlp(text)\n","  # Extract tokens for the given doc\n","  tokens = [token.lemma_ for token in doc if not (token.is_punct or token.is_space or token.is_stop) and (token.pos_=='NOUN' or token.pos_=='VERB')]\n","  return tokens"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1697683149886,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"vMvWE5BaD9W-"},"outputs":[],"source":["def compute_metrics(predicted_y, true_y, metric_function, columns, limit):\n","  results = pd.DataFrame(columns = columns)\n","  results.loc[len(results)] = metric_function(true_y, predicted_y, average=None)\n","  sorted_results = results.sort_values(by=0, axis=1, ascending=False)\n","  return sorted_results.iloc[:, :limit]"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697683149886,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"NhfYlODBD_QH"},"outputs":[],"source":["def print_confusion_matrix(cf_matrix, name):\n","  ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n","\n","  ax.set_title(name + ' Confusion Matrix \\n\\n');\n","  ax.set_xlabel('\\nPredicted Values')\n","  ax.set_ylabel('Actual Values ');\n","\n","  ## Ticket labels - List must be in alphabetical order\n","  ax.xaxis.set_ticklabels(['False','True'])\n","  ax.yaxis.set_ticklabels(['False','True'])\n","\n","  ## Display the visualization of the Confusion Matrix.\n","  plt.show()"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697683149887,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"nTkITEh4EA-B"},"outputs":[],"source":["def print_F1_based_on_distribution(y_true, y_pred, Y, columns):\n","  fig,ax = plt.subplots()\n","\n","  results = pd.DataFrame(columns = columns)\n","  results.loc[len(results)] = f1_score(y_true, y_pred, average=None)\n","\n","  Y_count = Y.apply(np.sum, axis=0)\n","  Y_count_sorted = Y_count.sort_values(ascending=False)\n","\n","  ax.bar(Y_count_sorted.index, Y_count_sorted.values)\n","  ax.set_xlabel(\"Techniques\")\n","  ax.set_ylabel(\"Number of CVEs\")\n","  plt.xticks(rotation=90)\n","\n","  ax2=ax.twinx()\n","  ax2.plot(Y_count_sorted.index, results[Y_count_sorted.index].iloc[0], color='red')\n","  ax2.set_ylabel(\"F1 Score\")\n","\n","  ax = plt.gca()\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gkSCvZRFCTH9"},"source":["### Model Configuration"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["3b563a615d5c489098fdd0c4c3347783","41c6f981e4384119b168e5bd70dc9ffe","3e415f4d368148ef8b38edae62b52178","54cf9af245f54cef8d5f440c14b0edcc","bff409642e2846fa83d786e6de6e7e18","28391327ceb942b8b1e75041d1a71156","dc4322957d194f6ea3a236cf20839204","e1560962e85446b58f68287501e8efaa","b0734e506ed24db1b79c76a01495aba3","c4acdf02b0df414d95de351dd5044ec5","b3467e10218c449bbdd14956003f1d79","e1d9f91cb5f34e14b1ba62a574324667","26e273d48df44ea3b58794c90fc4b38b","5848680153c744fe9f43330d00cfed96","0becb1980cf74c2ea1a822651f11a4d1","b7408dd4e26e4e098915612713ec9de8","990f41a55efe40d48abd260908126e45","f3b11318a7164646baf0d6159998706c","1e34ec3034e44f4ab2a74feec726191e","906026f1ac9e4a7faccadab5a36b4134","f559619bd26148078c4e614e3f17abb9","8c1980215d47470ab7d00c5b986b98a6"]},"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1697683150936,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"TOCpDLHvCXcY","outputId":"a0ea1f40-8f90-422c-ede4-806d35861957"},"outputs":[],"source":["class Config:\n","  def __init__(self):\n","    super(Config, self).__init__()\n","\n","    self.SEED = 42\n","    self.MODEL_PATH = 'jackaduma/SecBERT'\n","    self.NUM_LABELS = 31\n","\n","    # data\n","    self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n","    self.MAX_LENGTH = 320\n","    self.BATCH_SIZE = 16\n","    self.VALIDATION_SPLIT = 0.25\n","\n","     # model\n","    self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    self.FULL_FINETUNING = True\n","    self.LR = 3e-5\n","    self.OPTIMIZER = 'AdamW'\n","    self.CRITERION = 'BCEWithLogitsLoss'\n","    self.N_VALIDATE_DUR_TRAIN = 3\n","    self.N_WARMUP = 0\n","    self.SAVE_BEST_ONLY = True\n","    self.EPOCHS = 50\n","\n","config = Config()"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1697683150936,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"NdGpQ8x2C2a5"},"outputs":[],"source":["def clean_abstract(text):\n","    text = text.split()\n","    text = [x.strip() for x in text]\n","    text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n","    text = ' '.join(text)\n","    text = re.sub('([.,!?()])', r' \\1 ', text)\n","    return text\n","\n","def get_texts(df):\n","    texts = df.apply(lambda x: clean_abstract(x))\n","    texts = texts.values.tolist()\n","    return texts\n","\n","class TransformerDataset(Dataset):\n","  def __init__(self, df, labels=None, set_type=None):\n","    super(TransformerDataset, self).__init__()\n","\n","    self.texts = get_texts(df)\n","\n","    self.set_type = set_type\n","    if self.set_type != 'test':\n","      self.labels = labels\n","\n","\n","    self.tokenizer = config.TOKENIZER\n","    self.max_length = config.MAX_LENGTH\n","\n","  def __len__(self):\n","      return len(self.texts)\n","\n","  def __getitem__(self, index):\n","    tokenized = self.tokenizer.encode_plus(\n","        self.texts[index],\n","        max_length=self.max_length,\n","        pad_to_max_length=True,\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_token_type_ids=False,\n","        return_tensors='pt'\n","    )\n","    input_ids = tokenized['input_ids'].squeeze()\n","    attention_mask = tokenized['attention_mask'].squeeze()\n","\n","    if self.set_type != 'test':\n","      return {\n","          'input_ids': input_ids.long(),\n","          'attention_mask': attention_mask.long(),\n","          'labels': torch.Tensor(self.labels[index]).float(),\n","      }\n","\n","    return {\n","        'input_ids': input_ids.long(),\n","        'attention_mask': attention_mask.long(),\n","    }"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":651,"status":"ok","timestamp":1697683151581,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"MZ7PyNXADEjV","outputId":"d779b3d3-5bda-44cd-fd5e-340c156f85a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_ids shape: torch.Size([16, 320])\n","attention_mask shape: torch.Size([16, 320])\n","labels shape: torch.Size([16, 31])\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["train_data = TransformerDataset(X_train['Text'], y_train.values)\n","val_data = TransformerDataset(X_test['Text'], y_test.values)\n","\n","train_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\n","val_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n","\n","b = next(iter(train_dataloader))\n","for k, v in b.items():\n","    print(f'{k} shape: {v.shape}')"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1697683151582,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"rJpME0IWDgPs"},"outputs":[],"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","\n","    self.transformer_model = AutoModel.from_pretrained(\n","        config.MODEL_PATH\n","    )\n","    self.dropout = nn.Dropout(0.5)\n","    self.output = nn.Linear(768, config.NUM_LABELS)\n","\n","  def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","    _, o2 = self.transformer_model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","        return_dict=False\n","    )\n","    x = self.dropout(o2)\n","    x = self.output(x)\n","\n","    return x"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1697683151583,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"tMmXAMD0Djee","outputId":"16dd4bf2-250f-47c2-d8da-0366e6cd7545"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["device = config.DEVICE\n","device"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1697683151583,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"lqjwL0wNDna-"},"outputs":[],"source":["def val(model, val_dataloader, criterion):\n","\n","    val_loss = 0\n","    true, pred = [], []\n","\n","    # set model.eval() every time during evaluation\n","    model.eval()\n","\n","    for step, batch in enumerate(val_dataloader):\n","        # unpack the batch contents and push them to the device (cuda or cpu).\n","        b_input_ids = batch['input_ids'].to(device)\n","        b_attention_mask = batch['attention_mask'].to(device)\n","        b_labels = batch['labels'].to(device)\n","\n","        # using torch.no_grad() during validation/inference is faster -\n","        # - since it does not update gradients.\n","        with torch.no_grad():\n","            # forward pass\n","            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n","\n","            # calculate loss\n","            loss = criterion(logits, b_labels)\n","            val_loss += loss.item()\n","            # since we're using BCEWithLogitsLoss, to get the predictions -\n","            # - sigmoid has to be applied on the logits first\n","            logits = torch.sigmoid(logits)\n","\n","            logits = np.round(logits.cpu().numpy())\n","\n","            labels = b_labels.cpu().numpy()\n","\n","            # the tensors are detached from the gpu and put back on -\n","            # - the cpu, and then converted to numpy in order to -\n","            # - use sklearn's metrics.\n","\n","            pred.extend(logits)\n","            true.extend(labels)\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print('Eval Val loss:', avg_val_loss)\n","    print('Eval Val accuracy:', accuracy_score(true, pred))\n","\n","\n","    val_micro_f1_score = f1_score(true, pred, average='micro')\n","    print('Eval Val micro f1 score:', val_micro_f1_score)\n","    return val_micro_f1_score\n","\n","def train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch):\n","\n","    # we validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n","    nv = config.N_VALIDATE_DUR_TRAIN\n","    temp = len(train_dataloader) // nv\n","\n","    if temp > 100:\n","      temp = temp - (temp % 100)\n","    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n","\n","    train_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader,\n","                                      desc='Epoch ' + str(epoch))):\n","        # set model.eval() every time during training\n","        model.train()\n","\n","        # unpack the batch contents and push them to the device (cuda or cpu).\n","        b_input_ids = batch['input_ids'].to(device)\n","        b_attention_mask = batch['attention_mask'].to(device)\n","        b_labels = batch['labels'].to(device)\n","\n","        # clear accumulated gradients\n","        optimizer.zero_grad()\n","\n","        # forward pass\n","        logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n","\n","        # calculate loss\n","        loss = criterion(logits, b_labels)\n","        train_loss += loss.item()\n","\n","        # backward pass\n","        loss.backward()\n","\n","        # update weights\n","        optimizer.step()\n","\n","        # update scheduler\n","        scheduler.step()\n","\n","        if step in validate_at_steps:\n","            print(f'-- Step: {step}')\n","            _ = val(model, val_dataloader, criterion)\n","\n","    avg_train_loss = train_loss / len(train_dataloader)\n","    print('Training loss:', avg_train_loss)"]},{"cell_type":"markdown","metadata":{"id":"iSiS02VfEKYC"},"source":["### Training Model"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ccc8f156323f4869abdfdf9a8f4705f4","af95e9b4638140bd965ed03d2ec607d1","ebe3e1fe0e7b4010b02ede34a2999e47","665355dd7a414ae6989c021c6e0d6624","11436964c3294feab16fb4a92d3c8bad","fb88d421f4ce404fb3c3bf5b46a2014b","648ecf3595e74ac7a608535c5cddab6a","dea2d4d4f7be438d92dd3d873d8e43f7","17b176bf8c9f4a6ca7997386cac8c691","25f0ee4948c848bd8e42c83722682cff","790e6aa8cd84434bba1c892a947122b3"]},"executionInfo":{"elapsed":4819,"status":"ok","timestamp":1697683156392,"user":{"displayName":"ismp 114梁宸熏","userId":"09068752513825087526"},"user_tz":-480},"id":"bWGvyWbIEM7Q","outputId":"428b4b70-6b58-4724-c2aa-ba9f8b7b3a01"},"outputs":[{"data":{"text/plain":["Model(\n","  (transformer_model): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(52000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(514, 768)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (output): Linear(in_features=768, out_features=31, bias=True)\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model = Model()\n","model.to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def run():\n","    # setting a seed ensures reproducible results.\n","    # seed may affect the performance too.\n","    torch.manual_seed(config.SEED)\n","\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","    # define the parameters to be optmized -\n","    # - and add regularization\n","    if config.FULL_FINETUNING:\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {\n","                \"params\": [\n","                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n","                ],\n","                \"weight_decay\": 0.001,\n","            },\n","            {\n","                \"params\": [\n","                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n","                ],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n","\n","    num_training_steps = len(train_dataloader) * config.EPOCHS\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_training_steps\n","    )\n","\n","    max_val_micro_f1_score = float('-inf')\n","    for epoch in range(config.EPOCHS):\n","        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n","        val_micro_f1_score = val(model, val_dataloader, criterion)\n","        print(\"Epoch \" + str(epoch) + \"/\" + str(config.EPOCHS) + \": F1 Score \" + str(val_micro_f1_score))\n","        if config.SAVE_BEST_ONLY:\n","            if val_micro_f1_score > max_val_micro_f1_score:\n","                best_model = copy.deepcopy(model)\n","                best_val_micro_f1_score = val_micro_f1_score\n","\n","                model_name = 'cve2attck'\n","                torch.save(best_model.state_dict(), model_name + '.pt')\n","\n","                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n","                max_val_micro_f1_score = val_micro_f1_score\n","\n","    return best_model, best_val_micro_f1_score"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["de8617ec79504e43aa7d255495a1ff49","800a82ab23d848bba33487028428d608","56391a6c4c1c451fa0773cebf05b7158","0eaff3a11ef24491b9d5f2194dd0716b","0f168f0f013e41f49c8108f58944b6e1","b6f69d1e64a94228b70a40733bc3de41","d0cefc9b35bd4c448e6ed0f2ad59ca39","ccc5eed5487b488e8c18c7e664dd74c6","964214b90b6443279d9a26521e5a955b","cc8e15d2db83460e91cabf7a8b7f3e5e","00195e308176439e9048373698b7d353","d9d4a264e0ee481585015d131f57c26c","9ee7912aeeea4693a87ec5ae77998a8d","0209e2b765ca4652a5053d0a16e114b7","95a948cac21a4f7ab314ac00164435b5","41830708d4ea4e19a403b8050ef7f6c1","86bfdbc735c0494eb3ceed635814146f","d014954f23544c43a86a0d31d686ee31","53d922eac605442f8703dd9d7669968f","6ed25f30794a4765b851c0f526086585","5e3364932dc848d6aeaf6795e54a9017","fa1e6771f613495dbdc2612e9a85967e","962aa5fb8b1b4da49174478d02983a99","edbae100761f48c68df2baa63a869796","0879223411dc4ce1b0bd4caf3fecabd8","ea1b36a5bd874b1c97d2de6b31d33817","6b2e0c14642448a39d967b6cd1df5526","3d6b626fdcbd497aa2142ca743bdb10e","a170b128eed94ee3bcf9a77966485bd0","a748e90377da4ae39e995ae9fb77b02d","bb5c6f59c5e04de6bfa7c58d1dbe3611","64f8d88904ca4ac2bab0b703c130bceb","385e30e1df61430fa5df5cfebae4c6b6","2f06f652c2f744e3b91f955a9d3db164","002f77d05c8f4576ae88a8ace37c3057","a411155f8bde4b2e8dd71c473b5d2329","c47e80be7ca645eeb4bc88fc138e60b1","594187a59cfc45c0b14c8cecc042c64d","02e235acba96402c8cfcded2b998b49c","2fdf8da2136f484492bfe666fd3749ad","1423a6e4f59a4695b372324d9a48dc9f","84602b0a49bc4860b70d40aea7fbeb92","43c0e3912b044db09b4a2eee03674640","1a069beffb3b4da99945ee8ff885a283"]},"id":"WP787jTfETpi","outputId":"36c148f9-8a94-4407-e41a-72a03c5ad636"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d525b3e916c4946a08043c412e65cd4","version_major":2,"version_minor":0},"text/plain":["Epoch 0:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.2767111949622631\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.23374323770403863\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.21955184489488602\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n","Training loss: 0.30911949084085577\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.21955184489488602\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n","Epoch 0/50: F1 Score 0.0\n","--- Best Model. Val loss: -inf -> 0.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfec489601d3410c82271583dbe145cc","version_major":2,"version_minor":0},"text/plain":["Epoch 1:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.21273799985647202\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.21067185923457146\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.20876192823052406\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n","Training loss: 0.21249666424358593\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.20876192823052406\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n","Epoch 1/50: F1 Score 0.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"277886a19dc7465f837dd8f8d73c2f07","version_major":2,"version_minor":0},"text/plain":["Epoch 2:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.20702919512987136\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.20567986592650414\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.2012811154127121\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n","Training loss: 0.20414304566733976\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.2012811154127121\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.0\n","Epoch 2/50: F1 Score 0.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c03c953a971848978468ca1c9278a39f","version_major":2,"version_minor":0},"text/plain":["Epoch 3:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19695923775434493\n","Eval Val accuracy: 0.0031545741324921135\n","Eval Val micro f1 score: 0.02588996763754045\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19599075019359588\n","Eval Val accuracy: 0.0\n","Eval Val micro f1 score: 0.006525285481239805\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.18990024998784066\n","Eval Val accuracy: 0.031545741324921134\n","Eval Val micro f1 score: 0.1187214611872146\n","Training loss: 0.1957297698539846\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.18990024998784066\n","Eval Val accuracy: 0.031545741324921134\n","Eval Val micro f1 score: 0.1187214611872146\n","Epoch 3/50: F1 Score 0.1187214611872146\n","--- Best Model. Val loss: 0.0 -> 0.1187214611872146\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0966832d6ee04f3f9ed78df99befc2ad","version_major":2,"version_minor":0},"text/plain":["Epoch 4:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.18749575242400168\n","Eval Val accuracy: 0.031545741324921134\n","Eval Val micro f1 score: 0.14670658682634732\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1890077717602253\n","Eval Val accuracy: 0.01892744479495268\n","Eval Val micro f1 score: 0.0916030534351145\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1821717232465744\n","Eval Val accuracy: 0.03470031545741325\n","Eval Val micro f1 score: 0.17441860465116277\n","Training loss: 0.18246639265733608\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1821717232465744\n","Eval Val accuracy: 0.03470031545741325\n","Eval Val micro f1 score: 0.17441860465116277\n","Epoch 4/50: F1 Score 0.17441860465116277\n","--- Best Model. Val loss: 0.1187214611872146 -> 0.17441860465116277\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e38f03c6226441ddb240473045e5cbb3","version_major":2,"version_minor":0},"text/plain":["Epoch 5:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.18069560453295708\n","Eval Val accuracy: 0.03470031545741325\n","Eval Val micro f1 score: 0.1879382889200561\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18425289765000344\n","Eval Val accuracy: 0.03470031545741325\n","Eval Val micro f1 score: 0.1282798833819242\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17530020400881768\n","Eval Val accuracy: 0.04100946372239748\n","Eval Val micro f1 score: 0.21971830985915491\n","Training loss: 0.16830608809695524\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17530020400881768\n","Eval Val accuracy: 0.04100946372239748\n","Eval Val micro f1 score: 0.21971830985915491\n","Epoch 5/50: F1 Score 0.21971830985915491\n","--- Best Model. Val loss: 0.17441860465116277 -> 0.21971830985915491\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0aeea8b811604dc3a326a8951f6c1689","version_major":2,"version_minor":0},"text/plain":["Epoch 6:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1732493594288826\n","Eval Val accuracy: 0.04416403785488959\n","Eval Val micro f1 score: 0.20621468926553674\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1805617943406105\n","Eval Val accuracy: 0.05362776025236593\n","Eval Val micro f1 score: 0.20789473684210527\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17283808588981628\n","Eval Val accuracy: 0.050473186119873815\n","Eval Val micro f1 score: 0.2527173913043478\n","Training loss: 0.15412382053978302\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17283808588981628\n","Eval Val accuracy: 0.050473186119873815\n","Eval Val micro f1 score: 0.2527173913043478\n","Epoch 6/50: F1 Score 0.2527173913043478\n","--- Best Model. Val loss: 0.21971830985915491 -> 0.2527173913043478\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbd4e4f7cd17405ca8d28dce6774d635","version_major":2,"version_minor":0},"text/plain":["Epoch 7:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.16943203136324883\n","Eval Val accuracy: 0.056782334384858045\n","Eval Val micro f1 score: 0.2417582417582418\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1755512747913599\n","Eval Val accuracy: 0.07570977917981073\n","Eval Val micro f1 score: 0.2538461538461539\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1727586269378662\n","Eval Val accuracy: 0.0694006309148265\n","Eval Val micro f1 score: 0.2845849802371541\n","Training loss: 0.14038167648455677\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1727586269378662\n","Eval Val accuracy: 0.0694006309148265\n","Eval Val micro f1 score: 0.2845849802371541\n","Epoch 7/50: F1 Score 0.2845849802371541\n","--- Best Model. Val loss: 0.2527173913043478 -> 0.2845849802371541\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e08f4c3eec54b6ead270cfeffbf7695","version_major":2,"version_minor":0},"text/plain":["Epoch 8:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1692164395004511\n","Eval Val accuracy: 0.07886435331230283\n","Eval Val micro f1 score: 0.286472148541114\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.17924552783370018\n","Eval Val accuracy: 0.09779179810725552\n","Eval Val micro f1 score: 0.29404617253948967\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17176297567784787\n","Eval Val accuracy: 0.07255520504731862\n","Eval Val micro f1 score: 0.27440633245382584\n","Training loss: 0.1283157502903658\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17176297567784787\n","Eval Val accuracy: 0.07255520504731862\n","Eval Val micro f1 score: 0.27440633245382584\n","Epoch 8/50: F1 Score 0.27440633245382584\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c358bad03fcb48f4a13a55fc65b5f4f4","version_major":2,"version_minor":0},"text/plain":["Epoch 9:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.16899758353829383\n","Eval Val accuracy: 0.0914826498422713\n","Eval Val micro f1 score: 0.30808729139922975\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.17955287732183933\n","Eval Val accuracy: 0.12302839116719243\n","Eval Val micro f1 score: 0.3185628742514971\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1706163588911295\n","Eval Val accuracy: 0.07886435331230283\n","Eval Val micro f1 score: 0.3225806451612903\n","Training loss: 0.11722433251493117\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1706163588911295\n","Eval Val accuracy: 0.07886435331230283\n","Eval Val micro f1 score: 0.3225806451612903\n","Epoch 9/50: F1 Score 0.3225806451612903\n","--- Best Model. Val loss: 0.2845849802371541 -> 0.3225806451612903\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f54e170a6789488883d8d96e5243eb7e","version_major":2,"version_minor":0},"text/plain":["Epoch 10:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.16951882652938366\n","Eval Val accuracy: 0.08832807570977919\n","Eval Val micro f1 score: 0.3308080808080808\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.17935234867036343\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.387308533916849\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1757161196321249\n","Eval Val accuracy: 0.07886435331230283\n","Eval Val micro f1 score: 0.3392226148409895\n","Training loss: 0.10878322808181538\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1757161196321249\n","Eval Val accuracy: 0.07886435331230283\n","Eval Val micro f1 score: 0.3392226148409895\n","Epoch 10/50: F1 Score 0.3392226148409895\n","--- Best Model. Val loss: 0.3225806451612903 -> 0.3392226148409895\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e496a6a40905421d8f57a345d9995be3","version_major":2,"version_minor":0},"text/plain":["Epoch 11:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17196600809693335\n","Eval Val accuracy: 0.08517350157728706\n","Eval Val micro f1 score: 0.30788804071246817\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1709528747946024\n","Eval Val accuracy: 0.12302839116719243\n","Eval Val micro f1 score: 0.39506172839506176\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17488518208265305\n","Eval Val accuracy: 0.10410094637223975\n","Eval Val micro f1 score: 0.3345280764635603\n","Training loss: 0.09934529101147371\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17488518208265305\n","Eval Val accuracy: 0.10410094637223975\n","Eval Val micro f1 score: 0.3345280764635603\n","Epoch 11/50: F1 Score 0.3345280764635603\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc009e1a29f54f9b960b2617ff48c84c","version_major":2,"version_minor":0},"text/plain":["Epoch 12:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17283806428313256\n","Eval Val accuracy: 0.10094637223974763\n","Eval Val micro f1 score: 0.3516483516483517\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.17426299676299095\n","Eval Val accuracy: 0.12302839116719243\n","Eval Val micro f1 score: 0.3991416309012875\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1768487773835659\n","Eval Val accuracy: 0.10094637223974763\n","Eval Val micro f1 score: 0.3412887828162291\n","Training loss: 0.09013754677246599\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1768487773835659\n","Eval Val accuracy: 0.10094637223974763\n","Eval Val micro f1 score: 0.3412887828162291\n","Epoch 12/50: F1 Score 0.3412887828162291\n","--- Best Model. Val loss: 0.3392226148409895 -> 0.3412887828162291\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"709ffe0079bd44128ea06576b8103342","version_major":2,"version_minor":0},"text/plain":["Epoch 13:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.16945284977555275\n","Eval Val accuracy: 0.12618296529968454\n","Eval Val micro f1 score: 0.3948598130841121\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1757568631321192\n","Eval Val accuracy: 0.11987381703470032\n","Eval Val micro f1 score: 0.402907580477674\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17219042479991914\n","Eval Val accuracy: 0.1167192429022082\n","Eval Val micro f1 score: 0.3617021276595745\n","Training loss: 0.08244954146006528\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17219042479991914\n","Eval Val accuracy: 0.1167192429022082\n","Eval Val micro f1 score: 0.3617021276595745\n","Epoch 13/50: F1 Score 0.3617021276595745\n","--- Best Model. Val loss: 0.3412887828162291 -> 0.3617021276595745\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3b84a69a6814c8691832d3794d7e26c","version_major":2,"version_minor":0},"text/plain":["Epoch 14:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17047457806766034\n","Eval Val accuracy: 0.13249211356466878\n","Eval Val micro f1 score: 0.4027303754266212\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18067673370242118\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.40603015075376886\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1746093887835741\n","Eval Val accuracy: 0.10410094637223975\n","Eval Val micro f1 score: 0.3687281213535589\n","Training loss: 0.07670838398968473\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1746093887835741\n","Eval Val accuracy: 0.10410094637223975\n","Eval Val micro f1 score: 0.3687281213535589\n","Epoch 14/50: F1 Score 0.3687281213535589\n","--- Best Model. Val loss: 0.3617021276595745 -> 0.3687281213535589\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4f1d9333df34e6a92f32e997dc44b4b","version_major":2,"version_minor":0},"text/plain":["Epoch 15:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17255725637078284\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.4032073310423826\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18056412860751153\n","Eval Val accuracy: 0.13249211356466878\n","Eval Val micro f1 score: 0.40084388185654013\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17681583873927592\n","Eval Val accuracy: 0.10725552050473186\n","Eval Val micro f1 score: 0.3823870220162225\n","Training loss: 0.07187696570420966\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17681583873927592\n","Eval Val accuracy: 0.10725552050473186\n","Eval Val micro f1 score: 0.3823870220162225\n","Epoch 15/50: F1 Score 0.3823870220162225\n","--- Best Model. Val loss: 0.3687281213535589 -> 0.3823870220162225\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc058cf937894ca782905a0e4387b596","version_major":2,"version_minor":0},"text/plain":["Epoch 16:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17478554584085942\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.3928170594837261\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18226904533803462\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.39915966386554624\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17506270557641984\n","Eval Val accuracy: 0.12302839116719243\n","Eval Val micro f1 score: 0.39999999999999997\n","Training loss: 0.06635399800451362\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17506270557641984\n","Eval Val accuracy: 0.12302839116719243\n","Eval Val micro f1 score: 0.39999999999999997\n","Epoch 16/50: F1 Score 0.39999999999999997\n","--- Best Model. Val loss: 0.3823870220162225 -> 0.39999999999999997\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca742b4f851e45e992f03417a2d0fd9d","version_major":2,"version_minor":0},"text/plain":["Epoch 17:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17499284781515598\n","Eval Val accuracy: 0.13249211356466878\n","Eval Val micro f1 score: 0.4063564131668559\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1805632695555687\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.41458333333333336\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.18008246272802353\n","Eval Val accuracy: 0.10410094637223975\n","Eval Val micro f1 score: 0.36926605504587157\n","Training loss: 0.060957527007250226\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.18008246272802353\n","Eval Val accuracy: 0.10410094637223975\n","Eval Val micro f1 score: 0.36926605504587157\n","Epoch 17/50: F1 Score 0.36926605504587157\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e3d0cce1cb343869d5abdcb04784663","version_major":2,"version_minor":0},"text/plain":["Epoch 18:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1752567831426859\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.4161073825503356\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.17844386100769044\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.41580041580041577\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17964801974594594\n","Eval Val accuracy: 0.11987381703470032\n","Eval Val micro f1 score: 0.3821805392731536\n","Training loss: 0.05655392603400876\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17964801974594594\n","Eval Val accuracy: 0.11987381703470032\n","Eval Val micro f1 score: 0.3821805392731536\n","Epoch 18/50: F1 Score 0.3821805392731536\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7e6d16db53844628dc482df9dcf5c65","version_major":2,"version_minor":0},"text/plain":["Epoch 19:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17735622823238373\n","Eval Val accuracy: 0.12618296529968454\n","Eval Val micro f1 score: 0.394526795895097\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18080782741308213\n","Eval Val accuracy: 0.13249211356466878\n","Eval Val micro f1 score: 0.41850683491062035\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1813622869551182\n","Eval Val accuracy: 0.1167192429022082\n","Eval Val micro f1 score: 0.375886524822695\n","Training loss: 0.05274080410599709\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1813622869551182\n","Eval Val accuracy: 0.1167192429022082\n","Eval Val micro f1 score: 0.375886524822695\n","Epoch 19/50: F1 Score 0.375886524822695\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cf203d026e54658bab5641113090a6e","version_major":2,"version_minor":0},"text/plain":["Epoch 20:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17564630545675755\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.42139737991266374\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18089713528752327\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.4103114930182599\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.17898189164698125\n","Eval Val accuracy: 0.12302839116719243\n","Eval Val micro f1 score: 0.40181611804767314\n","Training loss: 0.050063462171922715\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.17898189164698125\n","Eval Val accuracy: 0.12302839116719243\n","Eval Val micro f1 score: 0.40181611804767314\n","Epoch 20/50: F1 Score 0.40181611804767314\n","--- Best Model. Val loss: 0.39999999999999997 -> 0.40181611804767314\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1eaa1118008453198715e77166ba0d8","version_major":2,"version_minor":0},"text/plain":["Epoch 21:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17700292095541953\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4088888888888889\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18540121093392373\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4073672806067173\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.18178219199180604\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.3941109852774632\n","Training loss: 0.045688561832203586\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.18178219199180604\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.3941109852774632\n","Epoch 21/50: F1 Score 0.3941109852774632\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f953556f2f9040729fe1ea5f09a5ec16","version_major":2,"version_minor":0},"text/plain":["Epoch 22:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.17883371599018574\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.4084821428571429\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18531193844974042\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.4051724137931034\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.18204570449888707\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.40793825799338485\n","Training loss: 0.043593243169872196\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.18204570449888707\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.40793825799338485\n","Epoch 22/50: F1 Score 0.40793825799338485\n","--- Best Model. Val loss: 0.40181611804767314 -> 0.40793825799338485\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d8bcb4539fb4cb0942fff30cbe5a694","version_major":2,"version_minor":0},"text/plain":["Epoch 23:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.18102265000343323\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.41156840934371525\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18388866558670997\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.40834248079034025\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.18336755447089673\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4070021881838074\n","Training loss: 0.041254403387360715\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.18336755447089673\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4070021881838074\n","Epoch 23/50: F1 Score 0.4070021881838074\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b181a19471c41cda0436ae6d5341875","version_major":2,"version_minor":0},"text/plain":["Epoch 24:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.18106785118579866\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.4049493813273341\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18493488542735576\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.412972972972973\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1844101704657078\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4066666666666666\n","Training loss: 0.039219729394158895\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1844101704657078\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4066666666666666\n","Epoch 24/50: F1 Score 0.4066666666666666\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a34f3d098d7d42df946999be51140aa3","version_major":2,"version_minor":0},"text/plain":["Epoch 25:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.18297878429293632\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.400895856662934\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1850729901343584\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4163969795037756\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.18755077831447126\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4008859357696567\n","Training loss: 0.0373499389299575\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.18755077831447126\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4008859357696567\n","Epoch 25/50: F1 Score 0.4008859357696567\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"997907faec7b49bdbf832b33d9a7beb8","version_major":2,"version_minor":0},"text/plain":["Epoch 26:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.18517175279557704\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.40358744394618834\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18501923009753227\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4168466522678186\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1907111469656229\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.4004376367614879\n","Training loss: 0.0354120845632518\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1907111469656229\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.4004376367614879\n","Epoch 26/50: F1 Score 0.4004376367614879\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28b1d5a6b1ca48dfb16a2a418015437c","version_major":2,"version_minor":0},"text/plain":["Epoch 27:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1868755638599396\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.4036281179138322\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18500561006367205\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.421505376344086\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1930431943386793\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4056399132321041\n","Training loss: 0.03416070874561282\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1930431943386793\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4056399132321041\n","Epoch 27/50: F1 Score 0.4056399132321041\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dad2ebc7b7ff43bca58472526ee67fad","version_major":2,"version_minor":0},"text/plain":["Epoch 28:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1860142719000578\n","Eval Val accuracy: 0.12618296529968454\n","Eval Val micro f1 score: 0.4085297418630752\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1844665262848139\n","Eval Val accuracy: 0.1608832807570978\n","Eval Val micro f1 score: 0.43524416135881105\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1917770016938448\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4134199134199134\n","Training loss: 0.03267953871365856\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1917770016938448\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4134199134199134\n","Epoch 28/50: F1 Score 0.4134199134199134\n","--- Best Model. Val loss: 0.40793825799338485 -> 0.4134199134199134\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43169cf4b2c1411babc42ce2eaf031f8","version_major":2,"version_minor":0},"text/plain":["Epoch 29:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1891827318817377\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.4026996625421822\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18694340884685517\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.4349680170575694\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19348633252084255\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4065217391304348\n","Training loss: 0.031446579591754606\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19348633252084255\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4065217391304348\n","Epoch 29/50: F1 Score 0.4065217391304348\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15cf7bbe863f4df08386381604091078","version_major":2,"version_minor":0},"text/plain":["Epoch 30:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19051711075007915\n","Eval Val accuracy: 0.13249211356466878\n","Eval Val micro f1 score: 0.4031531531531532\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.18711348362267016\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.4293419633225458\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19560500830411912\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.41530054644808745\n","Training loss: 0.03015530071056941\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19560500830411912\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.41530054644808745\n","Epoch 30/50: F1 Score 0.41530054644808745\n","--- Best Model. Val loss: 0.4134199134199134 -> 0.41530054644808745\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38248058056743fdaebfc2a93a21a7a2","version_major":2,"version_minor":0},"text/plain":["Epoch 31:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19053907431662082\n","Eval Val accuracy: 0.12618296529968454\n","Eval Val micro f1 score: 0.4009009009009009\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1876851424574852\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.4357066950053135\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19463348276913167\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.41855447680690394\n","Training loss: 0.02934537757845486\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19463348276913167\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.41855447680690394\n","Epoch 31/50: F1 Score 0.41855447680690394\n","--- Best Model. Val loss: 0.41530054644808745 -> 0.41855447680690394\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"721a65e85f8045bca088b1db0fa484e4","version_major":2,"version_minor":0},"text/plain":["Epoch 32:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1925107017159462\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.40765765765765766\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19044693149626254\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.41273326015367723\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1950729452073574\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.42826086956521736\n","Training loss: 0.0281006200293846\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1950729452073574\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.42826086956521736\n","Epoch 32/50: F1 Score 0.42826086956521736\n","--- Best Model. Val loss: 0.41855447680690394 -> 0.42826086956521736\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edad500a47e942238c61adf0d8829cc5","version_major":2,"version_minor":0},"text/plain":["Epoch 33:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1928497962653637\n","Eval Val accuracy: 0.13249211356466878\n","Eval Val micro f1 score: 0.40358744394618834\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19132014140486717\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.418859649122807\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19428037852048874\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.4322580645161291\n","Training loss: 0.027533569247187938\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19428037852048874\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.4322580645161291\n","Epoch 33/50: F1 Score 0.4322580645161291\n","--- Best Model. Val loss: 0.42826086956521736 -> 0.4322580645161291\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14d145afa1184c878499440f587aa625","version_major":2,"version_minor":0},"text/plain":["Epoch 34:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1950811915099621\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4151785714285714\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19228508397936822\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.4174863387978142\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19370963349938392\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4299465240641711\n","Training loss: 0.026193808851873175\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19370963349938392\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4299465240641711\n","Epoch 34/50: F1 Score 0.4299465240641711\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"296ffe3f675d40b689e02bffed1349d8","version_major":2,"version_minor":0},"text/plain":["Epoch 35:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19701421894133092\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4044692737430167\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1940402515232563\n","Eval Val accuracy: 0.12933753943217666\n","Eval Val micro f1 score: 0.407488986784141\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19480257332324982\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42701525054466233\n","Training loss: 0.025609926742446772\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19480257332324982\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42701525054466233\n","Epoch 35/50: F1 Score 0.42701525054466233\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"878fe40bee4d4a0d811bbdc26e32c731","version_major":2,"version_minor":0},"text/plain":["Epoch 36:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19731501303613186\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.4111731843575419\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1953653022646904\n","Eval Val accuracy: 0.12618296529968454\n","Eval Val micro f1 score: 0.4088397790055248\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.194435540959239\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4260869565217391\n","Training loss: 0.02489813510876368\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.194435540959239\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4260869565217391\n","Epoch 36/50: F1 Score 0.4260869565217391\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9150700ae47f4616bfddba1c267ec9d3","version_major":2,"version_minor":0},"text/plain":["Epoch 37:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19803538396954537\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4181415929203539\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19457707479596137\n","Eval Val accuracy: 0.13564668769716087\n","Eval Val micro f1 score: 0.41657579062159217\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19439227879047394\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.42779587404994573\n","Training loss: 0.024071941973970216\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19439227879047394\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.42779587404994573\n","Epoch 37/50: F1 Score 0.42779587404994573\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cedb52ae73142df940d0e6bd2442bdd","version_major":2,"version_minor":0},"text/plain":["Epoch 38:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1983872465789318\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4177777777777778\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19645020961761475\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4115044247787611\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19545373655855655\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.42686890574214514\n","Training loss: 0.02353019724216531\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19545373655855655\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.42686890574214514\n","Epoch 38/50: F1 Score 0.42686890574214514\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bf7709c60a941f091e936fb71c1344c","version_major":2,"version_minor":0},"text/plain":["Epoch 39:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.1989998571574688\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4177777777777778\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19694231823086739\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.4220994475138122\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19622337855398655\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.429035752979415\n","Training loss: 0.023172360192984343\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19622337855398655\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.429035752979415\n","Epoch 39/50: F1 Score 0.429035752979415\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c416f3fcfded44f39890598199b09b12","version_major":2,"version_minor":0},"text/plain":["Epoch 40:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.20006748251616954\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.41824249165739713\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.19832436479628085\n","Eval Val accuracy: 0.14195583596214512\n","Eval Val micro f1 score: 0.4186046511627907\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19663330353796482\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.42299349240780915\n","Training loss: 0.02253473197822185\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19663330353796482\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.42299349240780915\n","Epoch 40/50: F1 Score 0.42299349240780915\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46155af33da44c59a4c02225f9e6134b","version_major":2,"version_minor":0},"text/plain":["Epoch 41:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19909920394420624\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.41767955801104967\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.1988156683743\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.420353982300885\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.1985934939235449\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.420704845814978\n","Training loss: 0.021979478845263228\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.1985934939235449\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.420704845814978\n","Epoch 41/50: F1 Score 0.420704845814978\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"393087838c2b42fe8c3efcdcb2aff2aa","version_major":2,"version_minor":0},"text/plain":["Epoch 42:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.2006035890430212\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.41850220264317184\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.2005435697734356\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.41953385127635956\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19862741380929946\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.42562432138979367\n","Training loss: 0.021611759811639784\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19862741380929946\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.42562432138979367\n","Epoch 42/50: F1 Score 0.42562432138979367\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"701bdbaefdd9433d80696be5f55adf39","version_major":2,"version_minor":0},"text/plain":["Epoch 43:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.20120019428431987\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.41767955801104967\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.20076871216297149\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4128745837957824\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19888954609632492\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.4247020585048754\n","Training loss: 0.021406411253573263\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19888954609632492\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.4247020585048754\n","Epoch 43/50: F1 Score 0.4247020585048754\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7186bd2485d44794bec0219f98f31c8b","version_major":2,"version_minor":0},"text/plain":["Epoch 44:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19999224320054054\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.418859649122807\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.20191878266632557\n","Eval Val accuracy: 0.138801261829653\n","Eval Val micro f1 score: 0.4124860646599777\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.19932733438909053\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.42576419213973804\n","Training loss: 0.020947978804435802\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.19932733438909053\n","Eval Val accuracy: 0.15772870662460567\n","Eval Val micro f1 score: 0.42576419213973804\n","Epoch 44/50: F1 Score 0.42576419213973804\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4173eb63e86b48e5b0bcdfe7dfd5fe73","version_major":2,"version_minor":0},"text/plain":["Epoch 45:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.19973781295120716\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.4194260485651214\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.20131508260965347\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.41999999999999993\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.20089717768132687\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42464246424642466\n","Training loss: 0.02065406397313756\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.20089717768132687\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42464246424642466\n","Epoch 45/50: F1 Score 0.42464246424642466\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8af0bdf595e1451aadeb0c1c41a264cb","version_major":2,"version_minor":0},"text/plain":["Epoch 46:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.20040262266993522\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.4237102085620198\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.2017303306609392\n","Eval Val accuracy: 0.14511041009463724\n","Eval Val micro f1 score: 0.41731409544950054\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.2010586317628622\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.420704845814978\n","Training loss: 0.02083022741272169\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.2010586317628622\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.420704845814978\n","Epoch 46/50: F1 Score 0.420704845814978\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca56fb9e8ea8483f96506cc163f69c7a","version_major":2,"version_minor":0},"text/plain":["Epoch 47:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.2009050164371729\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.42024202420242024\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.20122173242270947\n","Eval Val accuracy: 0.14826498422712933\n","Eval Val micro f1 score: 0.420353982300885\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.20092738196253776\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4237102085620198\n","Training loss: 0.020408691153587663\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.20092738196253776\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4237102085620198\n","Epoch 47/50: F1 Score 0.4237102085620198\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe84e9b5f8af4a988d5e12fa571aeac9","version_major":2,"version_minor":0},"text/plain":["Epoch 48:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.2010213602334261\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.42244224422442245\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.20121517293155194\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42197802197802203\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.20126986168324948\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.4210526315789474\n","Training loss: 0.02051406779929119\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.20126986168324948\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.4210526315789474\n","Epoch 48/50: F1 Score 0.4210526315789474\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c23b7e86b04478e82e8d8ba17951542","version_major":2,"version_minor":0},"text/plain":["Epoch 49:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 28\n","Eval Val loss: 0.2012240994721651\n","Eval Val accuracy: 0.15141955835962145\n","Eval Val micro f1 score: 0.4205914567360351\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 56\n","Eval Val loss: 0.20139179974794388\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42151481888035125\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-- Step: 84\n","Eval Val loss: 0.20139568001031877\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42151481888035125\n","Training loss: 0.0204232232645154\n"]},{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Eval Val loss: 0.20139568001031877\n","Eval Val accuracy: 0.15457413249211358\n","Eval Val micro f1 score: 0.42151481888035125\n","Epoch 49/50: F1 Score 0.42151481888035125\n"]}],"source":["best_model, best_val_micro_f1_score = run()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# load\n","# model.load_state_dict(torch.load('cve2tech.pt'))"]},{"cell_type":"markdown","metadata":{"id":"lG_DASsnmC2y"},"source":["### Testing Model"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"EHWJe03vmF0_"},"outputs":[],"source":["test_data = TransformerDataset(our_X_test['Text'], y_test.values, set_type='test')\n","test_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"mcqWL3r8mLrp"},"outputs":[],"source":["def predict(model):\n","    val_loss = 0\n","    test_pred = []\n","    model.eval()\n","    for step, batch in enumerate(test_dataloader):\n","        b_input_ids = batch['input_ids'].to(device)\n","        b_attention_mask = batch['attention_mask'].to(device)\n","\n","        with torch.no_grad():\n","            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n","            logits = torch.sigmoid(logits)\n","            logits = np.round(logits.cpu().numpy())\n","            test_pred.extend(logits)\n","\n","    test_pred = np.array(test_pred)\n","    return test_pred"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"LveMfT3wmOvY"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ismp/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["test_pred = predict(best_model)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"DtSC8_9gVXKa"},"outputs":[{"name":"stdout","output_type":"stream","text":["       Process Injection  Access Token Manipulation  Hijack Execution Flow  \\\n","0                    0.0                        0.0                    0.0   \n","1                    0.0                        0.0                    0.0   \n","2                    0.0                        0.0                    0.0   \n","3                    0.0                        0.0                    0.0   \n","4                    0.0                        0.0                    0.0   \n","...                  ...                        ...                    ...   \n","89655                0.0                        0.0                    0.0   \n","89656                0.0                        0.0                    0.0   \n","89657                0.0                        0.0                    0.0   \n","89658                0.0                        0.0                    0.0   \n","89659                0.0                        0.0                    0.0   \n","\n","       Data from Local System  External Remote Services  Data Manipulation  \\\n","0                         0.0                       0.0                0.0   \n","1                         0.0                       0.0                0.0   \n","2                         0.0                       0.0                0.0   \n","3                         0.0                       0.0                0.0   \n","4                         0.0                       0.0                0.0   \n","...                       ...                       ...                ...   \n","89655                     0.0                       0.0                0.0   \n","89656                     0.0                       0.0                0.0   \n","89657                     0.0                       0.0                0.0   \n","89658                     0.0                       0.0                0.0   \n","89659                     0.0                       0.0                0.0   \n","\n","       Network Sniffing  Exploitation for Privilege Escalation  \\\n","0                   0.0                                    0.0   \n","1                   0.0                                    0.0   \n","2                   0.0                                    0.0   \n","3                   0.0                                    0.0   \n","4                   0.0                                    0.0   \n","...                 ...                                    ...   \n","89655               0.0                                    0.0   \n","89656               0.0                                    0.0   \n","89657               0.0                                    0.0   \n","89658               0.0                                    0.0   \n","89659               0.0                                    0.0   \n","\n","       Command and Scripting Interpreter  Phishing  ...  Valid Accounts  \\\n","0                                    0.0       0.0  ...             0.0   \n","1                                    0.0       0.0  ...             0.0   \n","2                                    0.0       0.0  ...             0.0   \n","3                                    1.0       0.0  ...             0.0   \n","4                                    1.0       0.0  ...             0.0   \n","...                                  ...       ...  ...             ...   \n","89655                                0.0       0.0  ...             0.0   \n","89656                                0.0       0.0  ...             0.0   \n","89657                                0.0       0.0  ...             0.0   \n","89658                                0.0       0.0  ...             0.0   \n","89659                                0.0       0.0  ...             0.0   \n","\n","       Exploitation for Defense Evasion  Create Account  \\\n","0                                   0.0             0.0   \n","1                                   0.0             0.0   \n","2                                   0.0             0.0   \n","3                                   0.0             0.0   \n","4                                   0.0             0.0   \n","...                                 ...             ...   \n","89655                               0.0             0.0   \n","89656                               0.0             0.0   \n","89657                               0.0             0.0   \n","89658                               0.0             0.0   \n","89659                               0.0             0.0   \n","\n","       Endpoint Denial of Service  Drive-by Compromise\\t  \\\n","0                             0.0                    0.0   \n","1                             0.0                    0.0   \n","2                             0.0                    0.0   \n","3                             0.0                    0.0   \n","4                             0.0                    0.0   \n","...                           ...                    ...   \n","89655                         1.0                    0.0   \n","89656                         1.0                    0.0   \n","89657                         1.0                    0.0   \n","89658                         1.0                    0.0   \n","89659                         0.0                    0.0   \n","\n","       Exploitation for Client Execution  Exploitation of Remote Services  \\\n","0                                    0.0                              0.0   \n","1                                    0.0                              0.0   \n","2                                    0.0                              0.0   \n","3                                    0.0                              0.0   \n","4                                    0.0                              0.0   \n","...                                  ...                              ...   \n","89655                                0.0                              0.0   \n","89656                                0.0                              0.0   \n","89657                                0.0                              0.0   \n","89658                                0.0                              0.0   \n","89659                                0.0                              0.0   \n","\n","       Stage Capabilities  Exploit Public-Facing Application  \\\n","0                     0.0                                0.0   \n","1                     0.0                                0.0   \n","2                     0.0                                0.0   \n","3                     0.0                                1.0   \n","4                     0.0                                1.0   \n","...                   ...                                ...   \n","89655                 0.0                                0.0   \n","89656                 0.0                                0.0   \n","89657                 0.0                                0.0   \n","89658                 0.0                                0.0   \n","89659                 0.0                                0.0   \n","\n","       Forge Web Credentials  \n","0                        0.0  \n","1                        0.0  \n","2                        0.0  \n","3                        0.0  \n","4                        0.0  \n","...                      ...  \n","89655                    0.0  \n","89656                    0.0  \n","89657                    0.0  \n","89658                    0.0  \n","89659                    0.0  \n","\n","[89660 rows x 31 columns]\n"]}],"source":["final_df = pd.DataFrame(test_pred, columns = ['Process Injection',\n","                               'Access Token Manipulation',\n","                               'Hijack Execution Flow',\n","                               'Data from Local System',\n","                               'External Remote Services',\n","                               'Data Manipulation',\n","                               'Network Sniffing',\n","                               'Exploitation for Privilege Escalation',\n","                               'Command and Scripting Interpreter',\n","                               'Phishing',\n","                               'Server Software Component',\n","                               'Archive Collected Data',\n","                               'Data Destruction',\n","                               'Browser Session Hijacking',\n","                               'Exploitation for Credential Access',\n","                               'Abuse Elevation Control Mechanism',\n","                               'Adversary-in-the-Middle',\n","                               'User Execution',\n","                               'Unsecured Credentials',\n","                               'Brute Force',\n","                               'File and Directory Discovery',\n","                               'Valid Accounts',\n","                               'Exploitation for Defense Evasion',\n","                               'Create Account',\n","                               'Endpoint Denial of Service',\n","                               'Drive-by Compromise\t',\n","                               'Exploitation for Client Execution',\n","                               'Exploitation of Remote Services',\n","                               'Stage Capabilities',\n","                               'Exploit Public-Facing Application',\n","                               'Forge Web Credentials',],index = None)\n","\n","print(final_df)\n","final_df.to_csv('our_y_test.csv',encoding = 'utf-8',index = None)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMNoCFQS82oaTOeQIn6wol3","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00195e308176439e9048373698b7d353":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"002f77d05c8f4576ae88a8ace37c3057":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02e235acba96402c8cfcded2b998b49c","placeholder":"​","style":"IPY_MODEL_2fdf8da2136f484492bfe666fd3749ad","value":"Epoch 3:  35%"}},"0209e2b765ca4652a5053d0a16e114b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53d922eac605442f8703dd9d7669968f","max":85,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ed25f30794a4765b851c0f526086585","value":85}},"02e235acba96402c8cfcded2b998b49c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0879223411dc4ce1b0bd4caf3fecabd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a748e90377da4ae39e995ae9fb77b02d","max":85,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb5c6f59c5e04de6bfa7c58d1dbe3611","value":85}},"0becb1980cf74c2ea1a822651f11a4d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f559619bd26148078c4e614e3f17abb9","placeholder":"​","style":"IPY_MODEL_8c1980215d47470ab7d00c5b986b98a6","value":" 378k/378k [00:00&lt;00:00, 2.89MB/s]"}},"0eaff3a11ef24491b9d5f2194dd0716b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc8e15d2db83460e91cabf7a8b7f3e5e","placeholder":"​","style":"IPY_MODEL_00195e308176439e9048373698b7d353","value":" 85/85 [49:51&lt;00:00, 76.78s/it]"}},"0f168f0f013e41f49c8108f58944b6e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11436964c3294feab16fb4a92d3c8bad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1423a6e4f59a4695b372324d9a48dc9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17b176bf8c9f4a6ca7997386cac8c691":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a069beffb3b4da99945ee8ff885a283":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e34ec3034e44f4ab2a74feec726191e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25f0ee4948c848bd8e42c83722682cff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26e273d48df44ea3b58794c90fc4b38b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_990f41a55efe40d48abd260908126e45","placeholder":"​","style":"IPY_MODEL_f3b11318a7164646baf0d6159998706c","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"28391327ceb942b8b1e75041d1a71156":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f06f652c2f744e3b91f955a9d3db164":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_002f77d05c8f4576ae88a8ace37c3057","IPY_MODEL_a411155f8bde4b2e8dd71c473b5d2329","IPY_MODEL_c47e80be7ca645eeb4bc88fc138e60b1"],"layout":"IPY_MODEL_594187a59cfc45c0b14c8cecc042c64d"}},"2fdf8da2136f484492bfe666fd3749ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"385e30e1df61430fa5df5cfebae4c6b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b563a615d5c489098fdd0c4c3347783":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41c6f981e4384119b168e5bd70dc9ffe","IPY_MODEL_3e415f4d368148ef8b38edae62b52178","IPY_MODEL_54cf9af245f54cef8d5f440c14b0edcc"],"layout":"IPY_MODEL_bff409642e2846fa83d786e6de6e7e18"}},"3d6b626fdcbd497aa2142ca743bdb10e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e415f4d368148ef8b38edae62b52178":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1560962e85446b58f68287501e8efaa","max":467,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0734e506ed24db1b79c76a01495aba3","value":467}},"41830708d4ea4e19a403b8050ef7f6c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41c6f981e4384119b168e5bd70dc9ffe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28391327ceb942b8b1e75041d1a71156","placeholder":"​","style":"IPY_MODEL_dc4322957d194f6ea3a236cf20839204","value":"Downloading (…)lve/main/config.json: 100%"}},"43c0e3912b044db09b4a2eee03674640":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53d922eac605442f8703dd9d7669968f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54cf9af245f54cef8d5f440c14b0edcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4acdf02b0df414d95de351dd5044ec5","placeholder":"​","style":"IPY_MODEL_b3467e10218c449bbdd14956003f1d79","value":" 467/467 [00:00&lt;00:00, 8.80kB/s]"}},"56391a6c4c1c451fa0773cebf05b7158":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccc5eed5487b488e8c18c7e664dd74c6","max":85,"min":0,"orientation":"horizontal","style":"IPY_MODEL_964214b90b6443279d9a26521e5a955b","value":85}},"5848680153c744fe9f43330d00cfed96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e34ec3034e44f4ab2a74feec726191e","max":378000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_906026f1ac9e4a7faccadab5a36b4134","value":378000}},"594187a59cfc45c0b14c8cecc042c64d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e3364932dc848d6aeaf6795e54a9017":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"648ecf3595e74ac7a608535c5cddab6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64f8d88904ca4ac2bab0b703c130bceb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"665355dd7a414ae6989c021c6e0d6624":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25f0ee4948c848bd8e42c83722682cff","placeholder":"​","style":"IPY_MODEL_790e6aa8cd84434bba1c892a947122b3","value":" 336M/336M [00:02&lt;00:00, 161MB/s]"}},"6b2e0c14642448a39d967b6cd1df5526":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ed25f30794a4765b851c0f526086585":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"790e6aa8cd84434bba1c892a947122b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"800a82ab23d848bba33487028428d608":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6f69d1e64a94228b70a40733bc3de41","placeholder":"​","style":"IPY_MODEL_d0cefc9b35bd4c448e6ed0f2ad59ca39","value":"Epoch 0: 100%"}},"84602b0a49bc4860b70d40aea7fbeb92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86bfdbc735c0494eb3ceed635814146f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c1980215d47470ab7d00c5b986b98a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"906026f1ac9e4a7faccadab5a36b4134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95a948cac21a4f7ab314ac00164435b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e3364932dc848d6aeaf6795e54a9017","placeholder":"​","style":"IPY_MODEL_fa1e6771f613495dbdc2612e9a85967e","value":" 85/85 [48:48&lt;00:00, 74.65s/it]"}},"962aa5fb8b1b4da49174478d02983a99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_edbae100761f48c68df2baa63a869796","IPY_MODEL_0879223411dc4ce1b0bd4caf3fecabd8","IPY_MODEL_ea1b36a5bd874b1c97d2de6b31d33817"],"layout":"IPY_MODEL_6b2e0c14642448a39d967b6cd1df5526"}},"964214b90b6443279d9a26521e5a955b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"990f41a55efe40d48abd260908126e45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ee7912aeeea4693a87ec5ae77998a8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86bfdbc735c0494eb3ceed635814146f","placeholder":"​","style":"IPY_MODEL_d014954f23544c43a86a0d31d686ee31","value":"Epoch 1: 100%"}},"a170b128eed94ee3bcf9a77966485bd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a411155f8bde4b2e8dd71c473b5d2329":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1423a6e4f59a4695b372324d9a48dc9f","max":85,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84602b0a49bc4860b70d40aea7fbeb92","value":30}},"a748e90377da4ae39e995ae9fb77b02d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af95e9b4638140bd965ed03d2ec607d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb88d421f4ce404fb3c3bf5b46a2014b","placeholder":"​","style":"IPY_MODEL_648ecf3595e74ac7a608535c5cddab6a","value":"Downloading model.safetensors: 100%"}},"b0734e506ed24db1b79c76a01495aba3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b3467e10218c449bbdd14956003f1d79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6f69d1e64a94228b70a40733bc3de41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7408dd4e26e4e098915612713ec9de8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb5c6f59c5e04de6bfa7c58d1dbe3611":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bff409642e2846fa83d786e6de6e7e18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c47e80be7ca645eeb4bc88fc138e60b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43c0e3912b044db09b4a2eee03674640","placeholder":"​","style":"IPY_MODEL_1a069beffb3b4da99945ee8ff885a283","value":" 30/85 [17:05&lt;59:34, 64.98s/it]"}},"c4acdf02b0df414d95de351dd5044ec5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc8e15d2db83460e91cabf7a8b7f3e5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc5eed5487b488e8c18c7e664dd74c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc8f156323f4869abdfdf9a8f4705f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af95e9b4638140bd965ed03d2ec607d1","IPY_MODEL_ebe3e1fe0e7b4010b02ede34a2999e47","IPY_MODEL_665355dd7a414ae6989c021c6e0d6624"],"layout":"IPY_MODEL_11436964c3294feab16fb4a92d3c8bad"}},"d014954f23544c43a86a0d31d686ee31":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0cefc9b35bd4c448e6ed0f2ad59ca39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9d4a264e0ee481585015d131f57c26c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ee7912aeeea4693a87ec5ae77998a8d","IPY_MODEL_0209e2b765ca4652a5053d0a16e114b7","IPY_MODEL_95a948cac21a4f7ab314ac00164435b5"],"layout":"IPY_MODEL_41830708d4ea4e19a403b8050ef7f6c1"}},"dc4322957d194f6ea3a236cf20839204":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de8617ec79504e43aa7d255495a1ff49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_800a82ab23d848bba33487028428d608","IPY_MODEL_56391a6c4c1c451fa0773cebf05b7158","IPY_MODEL_0eaff3a11ef24491b9d5f2194dd0716b"],"layout":"IPY_MODEL_0f168f0f013e41f49c8108f58944b6e1"}},"dea2d4d4f7be438d92dd3d873d8e43f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1560962e85446b58f68287501e8efaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1d9f91cb5f34e14b1ba62a574324667":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26e273d48df44ea3b58794c90fc4b38b","IPY_MODEL_5848680153c744fe9f43330d00cfed96","IPY_MODEL_0becb1980cf74c2ea1a822651f11a4d1"],"layout":"IPY_MODEL_b7408dd4e26e4e098915612713ec9de8"}},"ea1b36a5bd874b1c97d2de6b31d33817":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64f8d88904ca4ac2bab0b703c130bceb","placeholder":"​","style":"IPY_MODEL_385e30e1df61430fa5df5cfebae4c6b6","value":" 85/85 [50:33&lt;00:00, 86.44s/it]"}},"ebe3e1fe0e7b4010b02ede34a2999e47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea2d4d4f7be438d92dd3d873d8e43f7","max":336396808,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17b176bf8c9f4a6ca7997386cac8c691","value":336396808}},"edbae100761f48c68df2baa63a869796":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d6b626fdcbd497aa2142ca743bdb10e","placeholder":"​","style":"IPY_MODEL_a170b128eed94ee3bcf9a77966485bd0","value":"Epoch 2: 100%"}},"f3b11318a7164646baf0d6159998706c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f559619bd26148078c4e614e3f17abb9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa1e6771f613495dbdc2612e9a85967e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb88d421f4ce404fb3c3bf5b46a2014b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
